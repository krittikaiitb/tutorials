{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial covers some basic concepts of web scraping. In all the tutorials before, the datasets that you had to work with were provided by us. Sometimes, it's easy to directly download these datasets from some websites. But some annoying websites do not make it that easy for you! For instance, in the first tutorial, the Moons_and_planets.csv file was parsed from this Wikipedia page https://en.wikipedia.org/wiki/List_of_natural_satellites where there is no option to directly download the dataset :(\n",
    "In such instances, we use web scraping. It is a technique for extracting data from websites and storing it in a file on your computer. <br>\n",
    "\n",
    "First, let's go through what makes up a web page. When we visit a webpage, our browser sends a request to the web server called a **GET**\n",
    "request(because we are requesting the server to send us files). The server then sends back files that tells our browser what the website looks like. These files are of different types : \n",
    "\n",
    "1. HTML :  Has the main contents of the page \n",
    "2. CSS : Used to \"style\" the webpage and make it look good\n",
    "3. JS : Javascript files make the webpage more interactive\n",
    "4. Images : Helps add images to the webpage\n",
    "\n",
    "For our purposes, we need only concern ourselves with HTML, but you are free to look up the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML is not a programming language, it is a <i>markup language</i>, which means it tells the browser what the layout of the website looks like. It lets you do things like make a new heading, a new paragraph, make text bold, italicise text etc. HTML consists of elements called **tags** which basically gives the browser instructions like \"the following text is meant to be bold\" or \"the following text is another paragraph\". The most basic tag is the `<html>` tag. People who are already familiar with HTML can skip this section and move on to the actual web scraping part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<html>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells the browser that everything inside these two tags is HTML code. Ignore the first line, that is not part of an HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- This is how we comment in HTML :D  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given below are some common HTML tags.\n",
    "<l>\n",
    "    <li>`<head>` tag : Contains information about the title of the page</li>\n",
    "    <li>`<body>` tag : Contains information about the contents of the web page</li>\n",
    "    <li>`<p>` tag : Starts a new paragraph </li>\n",
    "    <li>`<a>` tag : Used to insert links in the webpage. The `href` property of this tag determines where the link goes. </li> \n",
    "</l> \n",
    "Run the code cell to see how HTML formats the page using these tags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML  \n",
    "<html>   \n",
    "\n",
    "    <head>                                    \n",
    "        This is <i>really</i> neat!           <!-- i tag : italics -->\n",
    "    </head>\n",
    "    \n",
    "    <body>                                    \n",
    "    \n",
    "        <p>\n",
    "        <b>This is <i>really</i> neat!</b>\n",
    "        </p>                                  <!-- b tag : bold -->\n",
    "                \n",
    "        <p>        \n",
    "        <a href = \"https://www.tech-iitb.org/krittika/\">\n",
    "        Krittika, the Astronomy Club of IIT Bombay </a>  \n",
    "        </p>\n",
    "    </body>\n",
    "    \n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tag is called a <b>child</b> tag when it is inside another tag. Similarly, the enclosing tag is called the <b>parent</b> tag. A tag is the <b>sibling</b> of another tag if it is enclosed inside the same parent tag. In the above example, the two p tags are the children of the body tag and are sibling tags, while the body tag is a parent tag for these two.<br>\n",
    "\n",
    "Some other very common HTML tags are `<div>`(helps in dividing the webpage into different areas), `<table>`(creates a table) and `<form>`(creates an input form). Refer to [this](https://developer.mozilla.org/en-US/docs/Web/HTML/Element) for a more detailed discussion on HTML tags.<br>\n",
    "\n",
    "One last important concept in HTML are the `class` and `id` properties. These are used to give HTML elements names, and they make it easier for us while web scraping. A single element can have multiple classes and a class can also be shared among multiple elements. However, an id is unique to an HTML element and cannot be used more than once in a single webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<html>   \n",
    "\n",
    "    <head>                                    \n",
    "        This is <i>really</i> neat!           \n",
    "    </head>\n",
    "    \n",
    "    <body>                                    \n",
    "    \n",
    "        <p class = \"neat\">                     <!-- This 'p' tag is part of 1 class-->\n",
    "        <b>This is <i>really</i> neat!</b>\n",
    "        </p>                                  \n",
    "                \n",
    "        <p class = \"neat very-neat\">            <!--This 'p' tag is part of 2 classes-->    \n",
    "        <a href = \"https://www.tech-iitb.org/krittika/\" id=\"very-very-neat\">\n",
    "        Krittika, the Astronomy Club of IIT Bombay </a>  \n",
    "        </p>\n",
    "    </body>\n",
    "    \n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, adding classes and ids do not change the website's layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the  requests Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape a webpage, we first download the HTML contents of the page and the **requests** library in Python lets us do that. There are different types of requests that we can send to the webpage, but here we will be using the `GET` request. Let's scrape a very simple webpage we have created [here](https://fathimazarin.github.io/simple.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get(\"https://fathimazarin.github.io/simple.html\")\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above code and get a response code of 200, it means that your page downloaded successfully, while codes starting with a 4 or a 5 indicates an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command prints out the HTML code of the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the BeautifulSoup library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup is a Python library that lets us parse HTML and extract whatever text that we want from it. This code loads the BeautifulSoup library and creates an instance of the BeautifulSoup class(we will be discussing classes in a later tutorial). This code uses the HTML parser(used generally for HTML documents that aren't well-formed, our example in this case :P )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the HTML code was printed out in a very messy way when we used page.content . The following command can be used to format the code in a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the information that we would want to extract is most probably inside the body tag in a <i>p</i> tag(the paragraph tags) or inside a table tag etc. The BeautifulSoup library has functions that help you directly search for these tags in the HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p') #What datatype does it return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have realized that the HTML code given above has multiple <i>p</i> tags, yet it returned only the first one. The soup.find_all() function returns a list of all occurrences of that particular tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this still returns HTML code with pesky HTML tags that you don't want in your parsed file. The get_text() function will help you out here. You cannot use the get_text() function on the list returned by find_all(). You will have to access each element and use the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('p')[1].get_text())     #Why do you think there is a difference in output ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krittika=soup.find_all('p')[1]\n",
    "krittika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krittika.find_all('a')  #The find_all() function can also be used to search for tags inside a parent tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krittika.find_all('a')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krittika.find_all('a')[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You still would not want those \\n cluttering up your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')[1].get_text().replace('\\n','')  #What do you think this code does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching by class and id\n",
    "Adding one argument to the find_all() function helps you search by class and id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', attrs={\"class\":\"neat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p',attrs={\"class\":\"very-neat\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(id=\"very-very-neat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on to actual webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we parsed a relatively simple file with few lines of code, while that is not the case for real life websites(Go to any random webpage on Google Chrome or Firefox and press Ctrl+u and check for yourself). It will be hard to find the exact location of the paragraph or the tag that you want in such a big code. In such scenarios, the concept of class and id that we discussed above becomes useful. Web scraping is all about finding the right tag to search for using the find() function. <br>\n",
    "\n",
    "Let us try scraping a much more longer webpage, say, a Wikipedia page [here](https://en.wikipedia.org/wiki/Lists_of_stars_by_constellation) to print the list of constellations. The constellations are listed as an unordered list, hence they can be found inside `<li></li>` tags(these tags are responsible for the bullets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://en.wikipedia.org/wiki/Lists_of_stars_by_constellation\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('li')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the list of constellations that we want starts from the 6th element in the above list and there are 88 constellations in total. Let us try printing it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find_all('li')[5].get_text())\n",
    "print(soup.find_all('li')[6].get_text())\n",
    "constellations=[]\n",
    "for element in soup.find_all('li')[5:93]:    #How 93?\n",
    "    constellations.append(element.get_text())\n",
    "    \n",
    "print(constellations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One final lesson in web scraping\n",
    "Not all website owners would appreciate a random person parsing through their website and collecting data. There are serious ethical concerns related to web scraping and you should always make sure before doing it that the owner is okay with this. In this tutorial, we have used only Wikipedia pages where web scraping is always allowed :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have understood so far, you are good to go! You must have realized how annoying it is to open the website, see how it is formatted, check if there are any errors, find the tag which contains your data etc. So for today's assignment, you will be given the code to parse the data and format it. The corresponding functions will be explained and you can use them directly in your code. <br>\n",
    "## Your assignment...\n",
    "...should you choose to accept it will be the following:\n",
    "1. Parse [this](https://en.wikipedia.org/wiki/Lists_of_stars_by_constellation) webpage for the RA and Dec of stars of each constellation, convert these coordinates to Cartesian coordinates and store them by constellation and plot them using matplotlib.\n",
    "2. Try to recreate the 'Moons_and_planets.csv' file(used in the first tutorial) from [this](https://en.wikipedia.org/wiki/List_of_natural_satellites) webpage. You can take inspiration from how tables are scraped in the get_map() function for Task 1. Do remember to remove commas and uncertainties in the radius measurement.\n",
    "\n",
    "You can use the following code in Task 1. The data in https://en.wikipedia.org/wiki/Lists_of_stars_by_constellation is not well-formatted and these functions will help in that.\n",
    "**The useful data to be extracted in Task 1 and Task 2 are stored in a HTML table under the class 'wikitable sortable' and you can directly search by class for both tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(ra_s, dec_s):\n",
    "    h_ind = ra_s.find('h')\n",
    "    m_ind = ra_s.find('m')\n",
    "    s_ind = ra_s.find('s')    \n",
    "    h = float(ra_s[:h_ind])\n",
    "    m = float(ra_s[(h_ind+1):m_ind])\n",
    "    s = float(ra_s[(m_ind+1):s_ind])\n",
    "    ra = h + m/60 + s/3600\n",
    "    if dec_s[0] == '+':\n",
    "        sign = 1\n",
    "    else:\n",
    "        sign = -1\n",
    "    d_ind = dec_s.find('°')\n",
    "    m_ind = dec_s.find('′')\n",
    "    s_ind = dec_s.find('″')\n",
    "    d = float(dec_s[1:d_ind])\n",
    "    m = float(dec_s[(d_ind+1):m_ind])\n",
    "    s = float(dec_s[(m_ind+1):s_ind])\n",
    "    dec = sign*(d + m/60 + s/3600)\n",
    "    return ra, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_coords() function is used to format the RA and Dec information of each star. Right Ascension is similar to longitude and is measured in hours, minutes, seconds while Declination is similar to latitude and is measured in degrees, minutes, seconds. The code parses data from the website as a string. This function converts the string to float and then returns the RA as hours and Declination as degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(constellation):\n",
    "    url = f'https://en.wikipedia.org/wiki/List_of_stars_in_{constellation}' #page gets downloaded according to constellation\n",
    "    r = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, 'lxml')  #Here, the lxml parser is used instead of HTML parser\n",
    "\n",
    "    tab = soup.find_all('table', attrs={'class':'wikitable sortable'})[0]   #To extract information from a wikipedia table\n",
    "                               \n",
    "    data = [[]]\n",
    "    for i in tab.find_all('tr'):   #searching in each row of the table ( 'tr' tag stands for row)\n",
    "        row = []                    #declaring empty row\n",
    "        for j in i.find_all('td'):  #'td' tag stands for a cell\n",
    "            row.append(j.get_text())   #add the text contents of each row to the list\n",
    "        data.append(row)\n",
    "\n",
    "    heads = []\n",
    "    for i in tab.find_all('tr')[:1]:\n",
    "        for j in i.find_all('th'):             #'th' tag stands for header cell\n",
    "            heads.append(j.get_text().strip('\\n'))\n",
    "\n",
    "    name_ind = heads.index('Name')\n",
    "    ra_ind = heads.index('RA')\n",
    "    dec_ind = heads.index('Dec')\n",
    "                                  \n",
    "    mag_ind = heads.index('vis.mag.')\n",
    "    \n",
    "    name = []\n",
    "    ra = []\n",
    "    dec = []\n",
    "    mag = []\n",
    "    for i in data[2:-2]:\n",
    "        name_string = i[name_ind]\n",
    "        try:                                             #The code first tries to run the code inside try\n",
    "            ra_string = i[ra_ind].replace('\\xa0', '')\n",
    "            dec_string = i[dec_ind].replace('\\xa0', '')   #These are code used to format the data\n",
    "            mag_string = i[mag_ind]                       \n",
    "            if mag_string[0]=='−':\n",
    "                mag_string = '-'+mag_string[1:]\n",
    "        except:                                       #If any error gets thrown up, it will execute the code inside except\n",
    "            continue\n",
    "        try:\n",
    "            ra_i, dec_i = get_coords(ra_string, dec_string)     #convert ra dec from string to float\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            mag.append(float(mag_string))\n",
    "            name.append(name_string)\n",
    "            ra.append(ra_i)\n",
    "            dec.append(dec_i)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    name = np.array(name)\n",
    "    ra = np.array(ra)\n",
    "    dec = np.array(dec)\n",
    "    mag = np.array(mag)\n",
    "    return name, ra, dec, mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function get_map() returns a numpy array of name of the stars in that particular constellation, the formatted RA, Dec coordinates for each star in hours and degrees respectively and the [apparent magnitudes](https://en.wikipedia.org/wiki/Apparent_magnitude). You might have noticed the use of `try` and `except` in the above code. These commands are used to handle errors while executing. The interpreter will first try to execute the code inside `try`. If an error gets thrown up during that execution(for instance, incorrect formatting), the code inside `except` will get executed. This makes sure that any star with any formatting errors in its data will be skipped. A more accurate function which does not skip such stars will be given in the solutions of this tutorial for anyone who wants a better method.<br>\n",
    "The lxml parser has been used in the above function. Different parsers give different results and you should always use the parser that works for you. lxml is one of the fastest parsers available.<br>\n",
    "\n",
    "For the next step, you will need to write a function that takes in the celestial coordinates(RA, Dec) and returns its projections into a Cartesian space. This is called a **[Stereographic projection](https://en.wikipedia.org/wiki/Stereographic_projection)**, where points on a sphere are projected on to a plane. A hint to approach this would be to first convert RA, Dec to spherical coordinates on a unit sphere and then apply stereographic projection formulae. <br>\n",
    "\n",
    "To plot the final figure, write a function plot() that takes in a constellation name and plots it. The size of the star must be proportional to its brightness or flux. In tutorial 2, there was a discussion on Magnitudes in Astronomy and its relation to flux, which might prove useful here. Do normalize the size values before using them. If the points come out too small, you can scale the size appropriately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
